from imblearn.over_sampling import SMOTE
from imblearn.ensemble import BalancedRandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import joblib
import numpy as np
import pandas as pd
from sklearn.utils.class_weight import compute_class_weight
from xgboost import XGBClassifier
import os
from tqdm import tqdm
import seaborn as sns  # –î–ª—è —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã

class ModelTrainer:
    def __init__(self, data_path, model_output_path, target_col):
        self.data_path = data_path
        self.model_output_path = model_output_path
        self.target_col = target_col

    def load_data(self):
        print("[+] –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        df = pd.read_parquet(self.data_path)
        print(f"[+] Data loaded: {df.shape}")
        return df

    def preprocess(self, df, target_col):
        print("[+] –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

        # –£–¥–∞–ª—è–µ–º 'datetime', –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'datetime' in df.columns:
            df = df.drop(columns=['datetime'])

        # –û—Ç–¥–µ–ª—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        X = df.drop(columns=[target_col])
        y = df[target_col].astype(int)  # —É–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∫–ª–∞—Å—Å—ã ‚Äî —ç—Ç–æ int

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        print("[+] Target distribution in training set:")
        print(y_train.value_counts(normalize=True))

        print("[+] Target distribution in test set:")
        print(y_test.value_counts(normalize=True))

        return X_train, X_test, y_train, y_test

    def evaluate(self, model, X_test, y_test):
        print("[+] üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")
        y_pred = model.predict(X_test)

        print("[+] Classification Report:")
        print(classification_report(y_test, y_pred, digits=2))

        print("[+] Confusion Matrix:")
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.title("Confusion Matrix")
        plt.show()

    def plot_and_save_target_distribution(self, y_train, y_test):
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤, –µ—Å–ª–∏ –æ–Ω–∞ –µ—â–µ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs('plots', exist_ok=True)

        # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª—è y_train
        plt.figure(figsize=(6, 4))
        y_train.value_counts(normalize=True).sort_index().plot(kind='bar', title='Target distribution (train)')
        plt.ylabel('Proportion')
        plt.xlabel('Target class')
        plt.tight_layout()
        plt.savefig('plots/target_distribution_train.png')
        plt.close()

        # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª—è y_test
        plt.figure(figsize=(6, 4))
        y_test.value_counts(normalize=True).sort_index().plot(kind='bar', title='Target distribution (test)')
        plt.ylabel('Proportion')
        plt.xlabel('Target class')
        plt.tight_layout()
        plt.savefig('plots/target_distribution_test.png')
        plt.close()

        print("[+] Target distribution saved as PNG.")

    def train(self, X_train, y_train, class_weights):
        # –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTE –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ 1 –∏ 2
        smote = SMOTE(sampling_strategy='auto', random_state=42)
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º XGBoost –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ use_label_encoder –∏ scale_pos_weight
        model = XGBClassifier(
            objective='multi:softmax',
            num_class=len(np.unique(y_train)),
            eval_metric='mlogloss',  # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–µ—Ç—Ä–∏–∫–∞ —É–∫–∞–∑–∞–Ω–∞, –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
        )

        # –û–±–æ—Ä–∞—á–∏–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å tqdm –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        eval_set = [(X_train_resampled, y_train_resampled)]  # –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        model.fit(X_train_resampled, y_train_resampled, eval_set=eval_set, verbose=False)

        return model

    def save_model(self, model):
        joblib.dump(model, self.model_output_path)
        print(f"[+] Model saved to: {self.model_output_path}")

    def run(self):
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("[+] –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        df = self.load_data()

        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        print("[+] –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        X_train, X_test, y_train, y_test = self.preprocess(df, self.target_col)

        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
        print("[+] ‚öñÔ∏è –í—ã—á–∏—Å–ª–µ–Ω–∏–µ class_weights...")
        '''
        –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º –Ω–∞–±–æ—Ä–µ:
        	‚Ä¢	–ö–ª–∞—Å—Å 0 (–æ—Å–Ω–æ–≤–Ω–æ–π): 98.59% –¥–∞–Ω–Ω—ã—Ö.
        	‚Ä¢	–ö–ª–∞—Å—Å 1: 0.70% –¥–∞–Ω–Ω—ã—Ö.
        	‚Ä¢	–ö–ª–∞—Å—Å 2: 0.70% –¥–∞–Ω–Ω—ã—Ö.

        –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –≤ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ:
        	‚Ä¢	–ö–ª–∞—Å—Å 0: 98.58% –¥–∞–Ω–Ω—ã—Ö.
        	‚Ä¢	–ö–ª–∞—Å—Å 1: 0.75% –¥–∞–Ω–Ω—ã—Ö.
        	‚Ä¢	–ö–ª–∞—Å—Å 2: 0.67% –¥–∞–Ω–Ω—ã—Ö.
        	
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤:
	        ‚Ä¢	–í–µ—Å –¥–ª—è –∫–ª–∞—Å—Å–∞ 0: 0.34
	        ‚Ä¢	–í–µ—Å –¥–ª—è –∫–ª–∞—Å—Å–∞ 1: 47.37
	        ‚Ä¢	–í–µ—Å –¥–ª—è –∫–ª–∞—Å—Å–∞ 2: 47.40
	        ‚Ä¢	–≠—Ç–∏ –≤–µ—Å–∞ –±—ã–ª–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤, 
	            –∫–æ–º–ø–µ–Ω—Å–∏—Ä—É—è –º–µ–Ω—å—à—É—é –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤ 1 –∏ 2.  	
        '''
        classes = np.unique(y_train)
        weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
        class_weights = dict(zip(classes, weights))
        print(f"[+] Class weights: {class_weights}")

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
        self.plot_and_save_target_distribution(y_train, y_test)

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("[+] üß† –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ XGBoost...")
        model = self.train(X_train, y_train, class_weights)

        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        print("[+] üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")
        '''
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏:
	‚Ä¢	Classification Report:
	‚Ä¢	–ö–ª–∞—Å—Å 0 (–æ—Å–Ω–æ–≤–Ω–æ–π):
	‚Ä¢	Precision (—Ç–æ—á–Ω–æ—Å—Ç—å): 1.00 ‚Äî –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã –∫–ª–∞—Å—Å–∞ 0.
	‚Ä¢	Recall (–ø–æ–ª–Ω–æ—Ç–∞): 1.00 ‚Äî –º–æ–¥–µ–ª—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã –∫–ª–∞—Å—Å–∞ 0.
	‚Ä¢	F1-score: 1.00 ‚Äî –∏–¥–µ–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –∫–ª–∞—Å—Å–∞ 0.
	‚Ä¢	–ö–ª–∞—Å—Å 1:
	‚Ä¢	Precision: 0.92 ‚Äî –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç 92% –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Å–∞ 1.
	‚Ä¢	Recall: 1.00 ‚Äî –º–æ–¥–µ–ª—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã –∫–ª–∞—Å—Å–∞ 1.
	‚Ä¢	F1-score: 0.96 ‚Äî –¥–æ–≤–æ–ª—å–Ω–æ –≤—ã—Å–æ–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∫–ª–∞—Å—Å–∞ 1.
	‚Ä¢	–ö–ª–∞—Å—Å 2:
	‚Ä¢	Precision: 0.91 ‚Äî –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç 91% –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Å–∞ 2.
	‚Ä¢	Recall: 0.99 ‚Äî –º–æ–¥–µ–ª—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç 99% –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Å–∞ 2.
	‚Ä¢	F1-score: 0.95 ‚Äî –≤—ã—Å–æ–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∫–ª–∞—Å—Å–∞ 2.
	‚Ä¢	–û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏:
	‚Ä¢	Accuracy (—Ç–æ—á–Ω–æ—Å—Ç—å): 1.00 ‚Äî –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã (–≤–∫–ª—é—á–∞—è —Ä–µ–¥–∫–∏–µ –∫–ª–∞—Å—Å—ã).
	‚Ä¢	Macro average (—Å—Ä–µ–¥–Ω–µ–µ –ø–æ –∫–ª–∞—Å—Å–∞–º): 0.94 (–¥–ª—è precision), 1.00 (–¥–ª—è recall), 0.97 (–¥–ª—è f1-score)
	      ‚Äî —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –≤—Å–µ–º –∫–ª–∞—Å—Å–∞–º. –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, —É—á–∏—Ç—ã–≤–∞—è –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤.
	‚Ä¢	Weighted average (–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ): 1.00 ‚Äî —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤,
	     —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ–ª–Ω–æ—Ç—É –º–æ–¥–µ–ª–∏.
        '''
        self.evaluate(model, X_test, y_test)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print(f"[+] Model saved to: {self.model_output_path}")
        self.save_model(model)


